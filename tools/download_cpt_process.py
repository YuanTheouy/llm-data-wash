# -*- coding: utf-8 -*-
"""
é€šç”¨æ•°æ®é›†å¤„ç†ï¼šç²¾å‡†æŒ‡å®šå­æ•°æ®é›†+æµå¼ä¸‹è½½+å•æ–‡ä»¶æ¯”ä¾‹ä¸¥æ ¼50:25:15:10
nvidia/Nemotron-CC-Math-v1 ä»…ä»4pluså­æ•°æ®é›†é‡‡æ ·ï¼Œå…¶ä»–æ•°æ®é›†å‡æŒ‡å®šå¯¹åº”ç²¾å“å­é›†
è¾“å‡ºï¼š6ä¸ª10B Parquetï¼Œå•æ–‡ä»¶æ¯”ä¾‹ç²¾å‡†ï¼Œä¸æ¨èæ•°æ®é›†100%å¯¹é½
ä¼˜åŒ–ï¼šä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç¼“å­˜ï¼Œé¿å…60GBæ•°æ®å…¨éƒ¨åŠ è½½åˆ°å†…å­˜å¯¼è‡´OOM
"""
import os
import random
import math
import json
import shutil
from tqdm import tqdm
import pandas as pd
from datasets import load_dataset
import argparse

# å›ºå®šéšæœºç§å­ï¼Œä¿è¯é‡‡æ ·/æ‹†åˆ†å¯å¤ç°
random.seed(42)

# ==================== æ ¸å¿ƒé…ç½®ï¼ˆå·²ç²¾å‡†æŒ‡å®šæ‰€æœ‰ç›®æ ‡å­æ•°æ®é›†config_nameï¼‰====================
DATASET_CONFIGS = [
    {
        "name": "nvidia/Nemotron-CC-Math-v1",    # ä¸»æ•°æ®é›†ä»“åº“
        "config_name": "4plus",                  # ç²¾å‡†æŒ‡å®šï¼šä»…ä»4pluså­æ•°æ®é›†é‡‡æ ·
        "global_ratio": 0.5,                     # å…¨å±€50% â†’ å•æ–‡ä»¶5Bï¼Œæ€»30B
        "sample_type": "general_math",
        "text_fields": ["question", "answer", "text"],
    },
    {
        "name": "nvidia/Nemotron-Pretraining-Code-v1",  # ä¸»æ•°æ®é›†ä»“åº“
        "config_name": "synthetic_code_qa",             # ç²¾å‡†æŒ‡å®šï¼šåˆæˆä»£ç QAå­æ•°æ®é›†
        "global_ratio": 0.25,                           # å…¨å±€25% â†’ å•æ–‡ä»¶2.5Bï¼Œæ€»15B
        "sample_type": "general_code",
        "text_fields": ["question", "answer", "prompt", "completion", "text"],
    },
    {
        "name": "nvidia/Nemotron-CC-v2",         # ä¸»æ•°æ®é›†ä»“åº“
        "config_name": "High-Quality",           # ç²¾å‡†æŒ‡å®šï¼šHigh-Qualityå­æ•°æ®é›†
        "global_ratio": 0.15,                    # å…¨å±€15% â†’ å•æ–‡ä»¶1.5Bï¼Œæ€»9B
        "sample_type": "general_high_quality",
        "text_fields": ["text", "content", "sentence"],
    },
    {
        "name": "glaiveai/reasoning-v1-20m",     # ä¸»æ•°æ®é›†ä»“åº“
        "config_name": "default",                # ç²¾å‡†æŒ‡å®šï¼šdefaultå­æ•°æ®é›†
        "global_ratio": 0.10,                    # å…¨å±€10% â†’ å•æ–‡ä»¶1Bï¼Œæ€»6B
        "sample_type": "general_reasoning",
        "text_fields": ["question", "answer", "instruction", "output", "text"],
    },
]

# æ€»ç›®æ ‡ä¸æ‹†åˆ†é…ç½®ï¼ˆå›ºå®šï¼Œå•æ–‡ä»¶æ¯”ä¾‹ä¸¥æ ¼50:25:15:10ï¼‰
# ä¿®æ­£ï¼šç”¨æˆ·ç¡®è®¤ "60B" ä¸º 60 GiB ç‰©ç†å¤§å°
TOTAL_TARGET_BYTES = 60 * 1024**3  # å…¨å±€æ€»60 GiB
SPLIT_NUM = 6                      # æ‹†åˆ†ä¸º6ä¸ªæ–‡ä»¶
SINGLE_FILE_TOTAL = TOTAL_TARGET_BYTES / SPLIT_NUM  # å•ä¸ªæ–‡ä»¶10 GiB

# ç›®å½•ä¸è¿è¡Œé…ç½®
OUTPUT_DIR = "general_cpt_datasets_60B_exact_subset"
CACHE_DIR = "./hf_stream_cache_exact"
TEMP_DIR = "./temp_shards_cache"     # ä¸´æ—¶æ–‡ä»¶ç›®å½•ï¼Œç”¨äºç¼“è§£å†…å­˜å‹åŠ›
ESTIMATE_SAMPLE_CNT = 1000
STREAM_BATCH_SIZE = 1000
OUTPUT_PREFIX = "cpt_general_training_data_parquet_"

# é»˜è®¤é•œåƒåœ°å€ï¼ˆç”¨æˆ·å¯è¦†ç›–ï¼‰
DEFAULT_HF_ENDPOINT = "https://hf-mirror.com"

def parse_args():
    parser = argparse.ArgumentParser(description="ç²¾å‡†æŒ‡å®šå­æ•°æ®é›†-æµå¼å¤„ç†-å•æ–‡ä»¶æ¯”ä¾‹ä¸¥æ ¼")
    parser.add_argument("--output-dir", default=OUTPUT_DIR, type=str)
    parser.add_argument("--cache-dir", default=CACHE_DIR, type=str)
    parser.add_argument("--temp-dir", default=TEMP_DIR, type=str, help="ä¸´æ—¶æ–‡ä»¶å­˜å‚¨ç›®å½•")
    parser.add_argument("--estimate-samples", default=ESTIMATE_SAMPLE_CNT, type=int)
    parser.add_argument("--stream-batch-size", default=STREAM_BATCH_SIZE, type=int)
    parser.add_argument("--hf-token", type=str, help="Hugging Face Access Token (ä¹Ÿå¯é€šè¿‡ç¯å¢ƒå˜é‡ HF_TOKEN è®¾ç½®)")
    parser.add_argument("--hf-endpoint", default=DEFAULT_HF_ENDPOINT, type=str, help="Hugging Face é•œåƒåœ°å€ (ä¹Ÿå¯é€šè¿‡ç¯å¢ƒå˜é‡ HF_ENDPOINT è®¾ç½®)")
    return parser.parse_args()

def get_effective_text_field(example_keys, candidate_fields):
    """ä»ç¬¬ä¸€æ¡ç¤ºä¾‹åŒ¹é…æœ‰æ•ˆæ–‡æœ¬å­—æ®µï¼ˆæµå¼æ¨¡å¼ä¸“ç”¨ï¼‰"""
    for field in candidate_fields:
        if field in example_keys:
            return field
    for field in example_keys:
        if isinstance(example_keys[field], str):
            return field
    raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆæ–‡æœ¬å­—æ®µï¼Œè¯·æ£€æŸ¥é…ç½®")

def estimate_avg_sample_size(ds_config, cache_dir, estimate_cnt, token=None):
    """æµå¼é¢„é‡‡æ ·ï¼šä»…ä»æŒ‡å®šå­æ•°æ®é›†å–æ ·æœ¬ï¼Œä¼°ç®—å¹³å‡å­—èŠ‚å¤§å°"""
    ds_name = ds_config["name"]
    config_name = ds_config["config_name"]
    text_candidates = ds_config["text_fields"]
    
    ds_stream = load_dataset(
        ds_name,
        config_name=config_name,
        split="train",
        streaming=True,
        cache_dir=cache_dir,
        # trust_remote_code=True,  # å·²åºŸå¼ƒï¼Œç§»é™¤ä»¥é¿å…æŠ¥é”™
        token=token
    )
    first_example = next(iter(ds_stream))
    text_field = get_effective_text_field(first_example, text_candidates)
    
    ds_stream = load_dataset(
        ds_name,
        config_name=config_name,
        split="train",
        streaming=True,
        cache_dir=cache_dir,
        # trust_remote_code=True,  # å·²åºŸå¼ƒï¼Œç§»é™¤ä»¥é¿å…æŠ¥é”™
        token=token
    )
    total_bytes = 0
    valid_count = 0
    for example in ds_stream:
        text = example[text_field].strip()
        if not text:
            continue
        total_bytes += len(text.encode("utf-8"))
        valid_count += 1
        if valid_count >= estimate_cnt:
            break
    if valid_count == 0:
        raise ValueError(f"{ds_name}[{config_name}] å­æ•°æ®é›†æ— æœ‰æ•ˆæ ·æœ¬")
    avg_size = total_bytes / valid_count
    print(f"âœ… {ds_name}[{config_name}] é¢„é‡‡æ ·{valid_count}æ¡ï¼Œå¹³å‡å•æ ·æœ¬ï¼š{avg_size:.2f} Bytes")
    return avg_size, text_field

def stream_collect_dataset_to_temp(ds_config, cache_dir, temp_dir, estimate_cnt, token=None):
    """
    æµå¼é‡‡æ ·å¹¶å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼Œé¿å…å†…å­˜æº¢å‡º
    """
    ds_name = ds_config["name"]
    config_name = ds_config["config_name"]
    global_ratio = ds_config["global_ratio"]
    sample_type = ds_config["sample_type"]
    text_candidates = ds_config["text_fields"]
    
    global_target = int(TOTAL_TARGET_BYTES * global_ratio)
    single_shard_target = int(SINGLE_FILE_TOTAL * global_ratio)
    total_shard_target = [single_shard_target for _ in range(SPLIT_NUM)]
    total_shard_target[-1] = global_target - sum(total_shard_target[:-1])
    
    print(f"\n===== å¼€å§‹å¤„ç†ï¼š{ds_name} â†’ ã€{config_name}ã€‘å­æ•°æ®é›† =====")
    print(f"ğŸ“Œ å…¨å±€ç›®æ ‡ï¼š{global_target/1024**3:.2f}GB | å•åˆ†ç‰‡ç›®æ ‡ï¼š{single_shard_target/1024**3:.2f}GB")
    
    avg_sample_size, text_field = estimate_avg_sample_size(ds_config, cache_dir, estimate_cnt)
    shard_required_samples = [math.ceil((t / avg_sample_size) * 1.1) for t in total_shard_target]
    print(f"ğŸ“Œ å„åˆ†ç‰‡éœ€æœ‰æ•ˆæ ·æœ¬ï¼š{[f'{x:,}' for x in shard_required_samples]}")

    # å‡†å¤‡ä¸´æ—¶æ–‡ä»¶å¥æŸ„
    os.makedirs(temp_dir, exist_ok=True)
    temp_files = {} # {shard_idx: file_handle}
    temp_filenames = {} # {shard_idx: filename}
    
    for i in range(SPLIT_NUM):
        fname = os.path.join(temp_dir, f"{config_name}_shard_{i}.jsonl")
        temp_files[i] = open(fname, 'w', encoding='utf-8')
        temp_filenames[i] = fname

    # æ ¸å¿ƒï¼šstreaming=True å®ç°æµå¼ä¸‹è½½
    ds_stream = load_dataset(
        ds_name,
        config_name=config_name,
        split="train",
        streaming=True,  # å…³é”®ï¼šå¼€å¯æµå¼æ¨¡å¼
        cache_dir=cache_dir,
        # trust_remote_code=True,  # å·²åºŸå¼ƒï¼Œç§»é™¤ä»¥é¿å…æŠ¥é”™
        token=token
    )
    
    current_shard = 0
    collected_in_shard = 0
    pbar = tqdm(total=sum(shard_required_samples), desc=f"ğŸ”„ é‡‡æ ·{config_name}")

    try:
        for example in ds_stream:
            text = example[text_field].strip()
            if not text:
                continue
            
            sample = {"text": text, "sample_type": sample_type}
            # å†™å…¥å½“å‰åˆ†ç‰‡çš„ä¸´æ—¶æ–‡ä»¶
            temp_files[current_shard].write(json.dumps(sample, ensure_ascii=False) + "\n")
            
            collected_in_shard += 1
            pbar.update(1)
            
            if collected_in_shard >= shard_required_samples[current_shard]:
                print(f"\nâœ… {config_name} åˆ†ç‰‡{current_shard}å®Œæˆï¼ˆ{collected_in_shard:,}æ¡ï¼‰ï¼Œåˆ‡æ¢åˆ†ç‰‡{current_shard+1}")
                current_shard += 1
                collected_in_shard = 0
                # å…³é”®ï¼šè¾¾æ ‡å³åœï¼Œä¸å†ä¸‹è½½åç»­æ•°æ®
                if current_shard >= SPLIT_NUM:
                    print(f"ğŸ‰ {config_name} æ‰€æœ‰åˆ†ç‰‡æ”¶é›†å®Œæ¯•ï¼Œåœæ­¢ä¸‹è½½")
                    break
    finally:
        # å…³é—­æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
        for f in temp_files.values():
            f.close()
        pbar.close()

    return temp_filenames

def merge_temp_shards_and_save(all_ds_temp_files, output_dir, output_prefix):
    """
    è¯»å–ä¸´æ—¶æ–‡ä»¶ï¼Œåˆå¹¶ã€æ‰“ä¹±å¹¶ä¿å­˜ä¸ºParquet
    """
    os.makedirs(output_dir, exist_ok=True)
    print(f"\n===== åˆå¹¶åˆ†ç‰‡ â†’ 6ä¸ªæ–‡ä»¶ï¼ˆå•æ–‡ä»¶çº¦{SINGLE_FILE_TOTAL/1024**3:.2f}GBï¼‰ =====")
    
    for shard_idx in range(SPLIT_NUM):
        print(f"\nğŸ”„ æ­£åœ¨å¤„ç†åˆ†ç‰‡ {shard_idx}...")
        shard_all_samples = []
        
        # è¯»å–è¯¥åˆ†ç‰‡å¯¹åº”çš„æ‰€æœ‰å­æ•°æ®é›†ä¸´æ—¶æ–‡ä»¶
        for ds_config in DATASET_CONFIGS:
            config_name = ds_config["config_name"]
            temp_file = all_ds_temp_files[config_name][shard_idx]
            
            if os.path.exists(temp_file):
                print(f"   - è¯»å– {os.path.basename(temp_file)} ...")
                with open(temp_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            shard_all_samples.append(json.loads(line))
            else:
                print(f"   âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {temp_file}")

        # å•æ–‡ä»¶å†…æ‰“ä¹±
        print(f"   ğŸ”€ æ­£åœ¨æ‰“ä¹± {len(shard_all_samples):,} æ¡æ•°æ®...")
        random.shuffle(shard_all_samples)
        
        # è½¬æ¢ä¸ºDataFrame
        df_shard = pd.DataFrame(shard_all_samples)
        
        # ä¿å­˜
        output_file = os.path.join(output_dir, f"{output_prefix}{shard_idx:05d}.parquet")
        df_shard.to_parquet(output_file, index=False)
        
        # ç»Ÿè®¡
        actual_size = os.path.getsize(output_file)
        type_ratio = (df_shard["sample_type"].value_counts() / len(df_shard) * 100).round(2)
        
        print(f"âœ… æœ€ç»ˆæ–‡ä»¶{shard_idx+1}/{SPLIT_NUM}ï¼š{os.path.basename(output_file)}")
        print(f"   ğŸ“ å®é™…å¤§å°ï¼š{actual_size/1024**3:.2f}GB")
        print(f"    æ ·æœ¬æ€»æ•°ï¼š{len(df_shard):,}æ¡")
        print(f"   âš–ï¸  å†…éƒ¨æ¯”ä¾‹ï¼š")
        for tp, ratio in type_ratio.items():
            print(f"      {tp:20s}ï¼š{ratio:.2f}%")

    # å…¨å±€ç»Ÿè®¡
    total_size = sum(os.path.getsize(os.path.join(output_dir, f)) for f in os.listdir(output_dir) if f.endswith(".parquet"))
    print(f"\n===== æ‰€æœ‰æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼ =====")
    print(f"âœ… å…¨å±€æ€»å¤§å°ï¼š{total_size/1024**3:.2f}GBï¼ˆç›®æ ‡60GBï¼‰")

def main():
    args = parse_args()
    
    # è®¾ç½®ç¯å¢ƒå˜é‡ï¼šHF Mirror
    if args.hf_endpoint:
        os.environ["HF_ENDPOINT"] = args.hf_endpoint
        print(f"ğŸŒ ä½¿ç”¨ Hugging Face é•œåƒï¼š{os.environ['HF_ENDPOINT']}")
    
    # è·å– Tokenï¼šä¼˜å…ˆå‘½ä»¤è¡Œå‚æ•°ï¼Œå…¶æ¬¡ç¯å¢ƒå˜é‡
    hf_token = args.hf_token or os.environ.get("HF_TOKEN")
    if hf_token:
        print("ğŸ”‘ å·²æ£€æµ‹åˆ° Hugging Face Token")
    else:
        print("âš ï¸ æœªæ£€æµ‹åˆ° Tokenï¼Œéƒ¨åˆ†å—é™æ•°æ®é›†å¯èƒ½ä¼šä¸‹è½½å¤±è´¥")

    os.makedirs(args.cache_dir, exist_ok=True)
    os.makedirs(args.output_dir, exist_ok=True)
    os.makedirs(args.temp_dir, exist_ok=True)

    # æ­¥éª¤1ï¼šé€ä¸ªå¤„ç†æŒ‡å®šå­æ•°æ®é›†ï¼Œå†™å…¥ä¸´æ—¶æ–‡ä»¶
    all_ds_temp_files = {}  # {config_name: {shard_idx: temp_file_path}}
    
    for ds_config in DATASET_CONFIGS:
        try:
            temp_files_map = stream_collect_dataset_to_temp(
                ds_config=ds_config,
                cache_dir=args.cache_dir,
                temp_dir=args.temp_dir,
                estimate_cnt=args.estimate_samples,
                token=hf_token
            )
            all_ds_temp_files[ds_config["config_name"]] = temp_files_map
        except Exception as e:
            print(f"âŒ å¤„ç†{ds_config['name']}å¤±è´¥ï¼š{str(e)}")
            # å‘ç”Ÿé”™è¯¯æ—¶æ¸…ç†å·²ç”Ÿæˆçš„ä¸´æ—¶æ–‡ä»¶å¯èƒ½æ›´å¥½ï¼Œä½†ä¸ºäº†è°ƒè¯•ä¿ç•™
            raise e

    # æ­¥éª¤2ï¼šåˆå¹¶ä¸´æ—¶æ–‡ä»¶
    try:
        merge_temp_shards_and_save(
            all_ds_temp_files=all_ds_temp_files,
            output_dir=args.output_dir,
            output_prefix=OUTPUT_PREFIX
        )
    finally:
        # å¯é€‰ï¼šæ¸…ç†ä¸´æ—¶ç›®å½•
        # shutil.rmtree(args.temp_dir)
        print(f"\nâ„¹ï¸ ä¸´æ—¶æ–‡ä»¶ä¿ç•™åœ¨ {args.temp_dir}ï¼Œå¦‚éœ€é‡Šæ”¾ç©ºé—´è¯·æ‰‹åŠ¨åˆ é™¤")

if __name__ == "__main__":
    main()

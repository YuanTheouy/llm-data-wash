import json
import os
import threading
from typing import List, Dict, Optional
from llm_data_wash.core.base_generator import BaseGenerator
from llm_data_wash.utils.gpu_monitor import GPUMonitor
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer
from llm_data_wash.utils.logger import get_logger
from llm_data_wash.utils.concurrency import run_thread_pool
from tqdm import tqdm

logger = get_logger(__name__)

class VLLMGenerator(BaseGenerator):
    """å¤šå¡vLLMæ•°æ®é‡ç”Ÿæˆå™¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰ï¼šæ”¯æŒå¤šè½®æ‰¹é‡æ¨ç†+è¶…é•¿Promptå¤„ç†+å¼‚å¸¸å®¹é”™"""
    
    def __init__(self, config: Dict):
        super().__init__(config)
        
        # 1. ä¿æŒåŸæœ‰ç»“æ„è¯»å–
        self.vllm_config = config["vllm"]
        self.concurrency_config = config["concurrency"]
        
        # 2. åŸºç¡€è·¯å¾„å‚æ•° (ä¿æŒä¸å˜)
        self.model_path = self.vllm_config["model_path"]
        self.output_dir = config["data"]["output_dir"]
        
        # 3. æ˜ å°„å¹¶å‘å‚æ•° (å¤ç”¨åŸæœ‰å­—æ®µ)
        self.batch_size = self.concurrency_config["batch_size"]
        
        # 4. åˆå§‹åŒ– Tokenizer
        logger.info(f"æ­£åœ¨åŠ è½½ Tokenizer: {self.model_path}")
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path, 
            trust_remote_code=True
        )

        # 5. åˆå§‹åŒ– vLLM å¼•æ“ (å…¼å®¹æ—§ç‰ˆæœ¬vLLM)
        tp_size = self.vllm_config.get("tensor_parallel_size", 1)
        gpu_util = self.vllm_config.get("gpu_memory_utilization", 0.90)
        # æ ¸å¿ƒï¼šè¯»å–é…ç½®æ–‡ä»¶çš„8192ï¼ˆå·²æ”¹å¥½ï¼‰
        max_len = self.vllm_config.get("max_model_len", 8192)  
        # æ–°å¢ï¼šå¯é…ç½®çš„ç”Ÿæˆå‚æ•°å’Œé•¿åº¦é™åˆ¶
        self.max_generate_tokens = self.vllm_config.get("max_tokens", 512)
        self.max_allowed_rounds = self.vllm_config.get("max_allowed_rounds", 5)  # é™åˆ¶æœ€å¤§è½®æ¬¡
        self.single_turn_max_tokens = self.vllm_config.get("single_turn_max_tokens", 1024)  # å•è½®å†…å®¹é™åˆ¶
        
        logger.info(
            f"åˆå§‹åŒ– vLLM: TP={tp_size}, MemUtil={gpu_util}, MaxLen={max_len}, "
            f"MaxRounds={self.max_allowed_rounds}, SingleTurnMax={self.single_turn_max_tokens}"
        )
        
        # ========== å…¼å®¹æ—§ç‰ˆæœ¬vLLMçš„å¼•æ“åˆå§‹åŒ– ==========
        self.llm = LLM(
            model=self.model_path,
            tensor_parallel_size=tp_size,
            gpu_memory_utilization=gpu_util,
            max_model_len=max_len,
            trust_remote_code=True,
            dtype="auto",
            # å…¼å®¹æ—§ç‰ˆæœ¬çš„ä¼˜åŒ–å‚æ•°ï¼ˆåˆ é™¤enable_cuda_graphï¼‰
            enable_chunked_prefill=True,  # åˆ†å—å¤„ç†è¶…é•¿Prompt
            max_num_batched_tokens=8192,  # åŒ¹é…max_model_len
            max_num_seqs=2048,  # å¢å¤§å¹¶å‘åºåˆ—æ•°
            swap_space=4  # æ˜¾å­˜ä¸è¶³æ—¶ç”¨CPUå†…å­˜
        )
        
        # 6. é‡‡æ ·å‚æ•° (å¢å¼ºï¼šæ·»åŠ åœæ­¢è¯)
        self.sampling_params = SamplingParams(
            temperature=self.vllm_config["temperature"],
            max_tokens=self.max_generate_tokens,
            top_p=0.9,
            stop_token_ids=[self.tokenizer.eos_token_id] if self.tokenizer.eos_token_id else None
        )
        
        # æ‰¹é‡å¤„ç†å¤§å° (8å¡å»ºè®®è®¾ä¸º2000)
        self.process_batch_size = config["concurrency"].get("batch_size", 2000 if tp_size >=8 else 1000)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆç»å¯¹è·¯å¾„ï¼Œä¸ä¼šæ‰¾ä¸åˆ°ï¼‰
        os.makedirs(self.output_dir, exist_ok=True)
        # æ‰“å°ç»å¯¹è·¯å¾„ï¼Œç¡®ä¿ä½ èƒ½æ‰¾åˆ°
        logger.info(f"âœ… è¾“å‡ºç›®å½•å·²åˆ›å»ºï¼ˆç»å¯¹è·¯å¾„ï¼‰ï¼š{os.path.abspath(self.output_dir)}")
        print(f"\nğŸ“Œ æ•°æ®å°†ä¿å­˜åˆ°ï¼š{os.path.abspath(self.output_dir)}\n")

    def process_single(self, conversation: Dict, idx: int) -> Dict:
        """å¤„ç†å•æ¡å¯¹è¯ï¼ˆå®ç°æŠ½è±¡æ–¹æ³•ï¼Œå…¼å®¹åŸæœ‰æ¥å£ï¼‰"""
        # å…¼å®¹ä¸åŒå¯¹è¯æ ¼å¼
        convos = conversation.get("conversations",
                                conversation.get("items",
                                                conversation.get("messages", [])))
        new_convos = []
        context = ""
        
        for msg in convos:
            role = msg.get("from") or msg.get("role")
            content = msg.get("value") or msg.get("content")
            
            if role in ["human", "user"]:
                new_convos.append({"from": "human", "value": content})
                context += f"Human: {content}\n\n"
            elif role in ["gpt", "assistant"]:
                prompt = f"{context}Assistant:"
                new_answer = self.call_vllm_api(prompt)
                new_convos.append({"from": "gpt", "value": new_answer})
                context += f"Assistant: {new_answer}\n\n"
        
        return {
            "id": conversation.get("id", f"regenerated_{idx}"),
            "conversations": new_convos
        }
    
    def call_vllm_api(self, prompt: str) -> str:
        """å•æ¡Promptæ¨ç†ï¼ˆå…¼å®¹process_singleæ–¹æ³•ï¼‰"""
        try:
            # æ ¡éªŒPrompté•¿åº¦
            prompt_tokens = self.tokenizer.encode(prompt)
            max_prompt_len = self.llm.engine_config.max_model_len - self.max_generate_tokens
            if len(prompt_tokens) > max_prompt_len:
                prompt_tokens = prompt_tokens[-max_prompt_len:]
                prompt = self.tokenizer.decode(prompt_tokens, skip_special_tokens=False)
                logger.warning(f"å•æ¡Promptè¿‡é•¿ï¼Œå·²æˆªæ–­è‡³{max_prompt_len} Token")
            
            outputs = self.llm.generate([prompt], self.sampling_params)
            return outputs[0].outputs[0].text.strip()
        except Exception as e:
            logger.error(f"å•æ¡æ¨ç†å¤±è´¥: {str(e)[:200]}")
            return ""
    
    def process_batch(self, dataset: List[Dict]) -> List[Dict]:
        """
        æ ¸å¿ƒä¼˜åŒ–ç‰ˆï¼šæŒ‰è½®æ¬¡æ‰¹é‡æ¨ç†
        1. è¶…é•¿Promptè‡ªåŠ¨æˆªæ–­
        2. é™åˆ¶æœ€å¤§å¯¹è¯è½®æ¬¡/å•è½®é•¿åº¦
        3. å¼‚å¸¸æ•è·ï¼ˆå•æ ·æœ¬å¤±è´¥ä¸å½±å“æ•´æ‰¹ï¼‰
        4. æ˜¾å­˜é€‚é…8å¡ç¯å¢ƒ
        """
        total_count = len(dataset)
        logger.info(f"å¼€å§‹ç¦»çº¿æ¨ç†å¤„ç†ï¼šå…± {total_count} æ¡æ•°æ®ï¼Œæ‰¹æ¬¡å¤§å° {self.process_batch_size}")
        
        results = []
        total_batches = (total_count + self.process_batch_size - 1) // self.process_batch_size
        
        with tqdm(total=total_count, desc="æ•´ä½“å¤„ç†è¿›åº¦", unit="æ¡") as pbar:
            for batch_idx in range(total_batches):
                # 1. åˆ‡åˆ†å½“å‰æ‰¹æ¬¡
                start_idx = batch_idx * self.process_batch_size
                end_idx = min((batch_idx + 1) * self.process_batch_size, total_count)
                batch_data = dataset[start_idx:end_idx]
                current_batch_size = len(batch_data)
                
                # 2. é¢„å¤„ç†ï¼šæ‹†åˆ†å¤šè½®å¯¹è¯+åˆå§‹åŒ–å­˜å‚¨ç»“æ„
                batch_rounds = []          # æ¯ä¸ªæ ·æœ¬çš„è½®æ¬¡ä¿¡æ¯ [(user1, _), (user2, _)...]
                batch_chat_history = [[] for _ in range(current_batch_size)]  # å¯¹è¯å†å²
                batch_new_convs = [[] for _ in range(current_batch_size)]     # æœ€ç»ˆç”Ÿæˆçš„å¯¹è¯
                max_rounds = 0             # æ‰¹æ¬¡å†…æœ€å¤§è½®æ¬¡æ•°
                
                for sample_idx in range(current_batch_size):
                    conv_item = batch_data[sample_idx]
                    original_convs = conv_item.get("conversations", conv_item.get("items", []))
                    
                    # æ‹†åˆ†è½®æ¬¡ï¼ˆuser+assistantä¸ºä¸€è½®ï¼‰
                    rounds = []
                    current_user = None
                    for msg in original_convs:
                        role = msg.get("from") or msg.get("role")
                        content = msg.get("value") or msg.get("content")
                        
                        if role in ["human", "user"] and content.strip():
                            current_user = content.strip()
                        elif role in ["gpt", "assistant"] and current_user is not None:
                            rounds.append((current_user, content.strip()))
                            current_user = None
                    
                    # é™åˆ¶æœ€å¤§è½®æ¬¡ï¼ˆé¿å…å†å²è¿‡é•¿ï¼‰
                    if len(rounds) > self.max_allowed_rounds:
                        logger.warning(
                            f"æ ·æœ¬ {start_idx+sample_idx} è½®æ¬¡è¿‡å¤šï¼ˆ{len(rounds)}â†’{self.max_allowed_rounds}ï¼‰ï¼Œå·²æˆªæ–­"
                        )
                        rounds = rounds[-self.max_allowed_rounds:]
                    
                    batch_rounds.append(rounds)
                    max_rounds = max(max_rounds, len(rounds))
                
                # 3. æŒ‰è½®æ¬¡æ‰¹é‡æ¨ç†ï¼ˆæ ¸å¿ƒä¼˜åŒ–ï¼‰
                for round_idx in range(max_rounds):
                    round_prompts = []         # å½“å‰è½®æ¬¡æ‰€æœ‰æ ·æœ¬çš„Prompt
                    round_sample_indices = []  # æœ‰å½“å‰è½®æ¬¡çš„æ ·æœ¬ç´¢å¼•
                    
                    # æ„é€ å½“å‰è½®æ¬¡çš„æ‰¹é‡Prompt
                    for sample_idx in range(current_batch_size):
                        rounds = batch_rounds[sample_idx]
                        if round_idx >= len(rounds):
                            continue
                        
                        # æå–å½“å‰è½®æ¬¡userå†…å®¹+æˆªæ–­å•è½®è¶…é•¿å†…å®¹
                        user_content, _ = rounds[round_idx]
                        user_tokens = self.tokenizer.encode(user_content)
                        if len(user_tokens) > self.single_turn_max_tokens:
                            user_content = self.tokenizer.decode(
                                user_tokens[:self.single_turn_max_tokens], 
                                skip_special_tokens=True
                            )
                            logger.warning(
                                f"æ ·æœ¬ {start_idx+sample_idx} ç¬¬{round_idx}è½®Userå†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­"
                            )
                        
                        # æ„é€ Promptï¼ˆå†å²+å½“å‰userï¼‰
                        current_chat = batch_chat_history[sample_idx].copy()
                        current_chat.append({"role": "user", "content": user_content})
                        
                        # åº”ç”¨Chat Template
                        prompt_str = self.tokenizer.apply_chat_template(
                            current_chat,
                            tokenize=False,
                            add_generation_prompt=True
                        )
                        
                        # æ ¸å¿ƒä¿®å¤ï¼šæ ¡éªŒå¹¶æˆªæ–­è¶…é•¿Prompt
                        prompt_tokens = self.tokenizer.encode(prompt_str)
                        # é¢„ç•™ç”Ÿæˆç©ºé—´ï¼šmax_model_len - ç”Ÿæˆtokenæ•°
                        max_prompt_len = self.llm.engine_config.max_model_len - self.max_generate_tokens
                        if len(prompt_tokens) > max_prompt_len:
                            truncated_tokens = prompt_tokens[-max_prompt_len:]
                            prompt_str = self.tokenizer.decode(truncated_tokens, skip_special_tokens=False)
                            logger.warning(
                                f"æ ·æœ¬ {start_idx+sample_idx} ç¬¬{round_idx}è½®Promptè¿‡é•¿ "
                                f"ï¼ˆ{len(prompt_tokens)}â†’{max_prompt_len}ï¼‰ï¼Œå·²æˆªæ–­"
                            )
                        
                        round_prompts.append(prompt_str)
                        round_sample_indices.append(sample_idx)
                    
                    # 4. æ‰¹é‡æ¨ç†ï¼ˆå¸¦å¼‚å¸¸æ•è·ï¼‰
                    outputs = []
                    if round_prompts:
                        try:
                            # æ‰¹é‡æ¨ç†ï¼ˆ8å¡æ ¸å¿ƒä¼˜åŠ¿ï¼‰
                            outputs = self.llm.generate(round_prompts, self.sampling_params)
                        except Exception as e:
                            logger.error(
                                f"æ‰¹æ¬¡{batch_idx}ç¬¬{round_idx}è½®æ‰¹é‡æ¨ç†å¤±è´¥ï¼š{str(e)[:200]}"
                            )
                            # å…œåº•ï¼šé€ä¸ªæ¨ç†å¤±è´¥çš„æ ·æœ¬
                            outputs = []
                            for prompt in round_prompts:
                                try:
                                    single_output = self.llm.generate([prompt], self.sampling_params)
                                    outputs.append(single_output[0])
                                except Exception as e2:
                                    logger.error(f"å•æ ·æœ¬æ¨ç†å¤±è´¥ï¼š{str(e2)[:200]}")
                                    outputs.append(None)
                    
                    # 5. å›å¡«å½“å‰è½®æ¬¡ç»“æœ
                    for output_idx, output in enumerate(outputs):
                        sample_idx = round_sample_indices[output_idx]
                        if output is None or not output.outputs:
                            # å¤±è´¥å…œåº•ï¼šç©ºå›å¤
                            new_assistant = ""
                            logger.warning(
                                f"æ ·æœ¬ {start_idx+sample_idx} ç¬¬{round_idx}è½®æ¨ç†å¤±è´¥ï¼Œä½¿ç”¨ç©ºå›å¤"
                            )
                        else:
                            new_assistant = output.outputs[0].text.strip()
                        
                        # æ›´æ–°å¯¹è¯å†å²ï¼ˆç”¨äºä¸‹ä¸€è½®Promptï¼‰
                        current_user = batch_rounds[sample_idx][round_idx][0]
                        batch_chat_history[sample_idx].append({"role": "user", "content": current_user})
                        batch_chat_history[sample_idx].append({"role": "assistant", "content": new_assistant})
                        
                        # æ›´æ–°æœ€ç»ˆå¯¹è¯ç»“æ„
                        batch_new_convs[sample_idx].append({"from": "human", "value": current_user})
                        batch_new_convs[sample_idx].append({"from": "gpt", "value": new_assistant})
                
                # 6. æ„é€ å½“å‰æ‰¹æ¬¡ç»“æœ
                batch_results = []
                for sample_idx in range(current_batch_size):
                    conv_item = batch_data[sample_idx]
                    batch_results.append({
                        "id": conv_item.get("id", f"regenerated_{start_idx+sample_idx}"),
                        "conversations": batch_new_convs[sample_idx]
                    })
                
                # 7. ä¿å­˜å½“å‰æ‰¹æ¬¡
                self.save_batch(batch_results, batch_idx)
                results.extend(batch_results)
                pbar.update(current_batch_size)
        
        # 8. åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
        self._merge_batches(total_batches)
        logger.info(f"æ‰€æœ‰æ•°æ®æ¨ç†å®Œæˆï¼å…±ç”Ÿæˆ {len(results)} æ¡æ•°æ®ï¼Œè¾“å‡ºç›®å½•ï¼š{self.output_dir}")
        return results
    
    def save_batch(self, batch_data: List[Dict], batch_idx: int):
        """ä¿å­˜æ‰¹æ¬¡æ•°æ®ï¼ˆå¢å¼ºï¼šå¼‚å¸¸å¤„ç†+ä¸­æ–‡å…¼å®¹ï¼‰"""
        try:
            batch_file = os.path.join(self.output_dir, f"batch_{batch_idx:04d}.jsonl")
            with open(batch_file, "w", encoding="utf-8") as f:
                for item in batch_data:
                    f.write(json.dumps(item, ensure_ascii=False) + "\n")
            logger.info(f"æ‰¹æ¬¡ {batch_idx} ä¿å­˜å®Œæˆï¼š{len(batch_data)}æ¡ | è·¯å¾„ï¼š{batch_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜æ‰¹æ¬¡{batch_idx}å¤±è´¥ï¼š{e}")
    
    def _merge_batches(self, total_batches: int):
        """åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ï¼ˆå¢å¼ºï¼šæ¸…ç†ä¸´æ—¶æ–‡ä»¶+åŒæ ¼å¼ä¿å­˜ï¼‰"""
        try:
            # 1. åˆå¹¶ä¸ºJSONLï¼ˆæ¨èæ ¼å¼ï¼Œé€‚åˆå¤§æ–‡ä»¶ï¼‰
            merge_jsonl = os.path.join(self.output_dir, "regenerated_complete.jsonl")
            with open(merge_jsonl, "w", encoding="utf-8") as out_f:
                for batch_idx in range(total_batches):
                    batch_file = os.path.join(self.output_dir, f"batch_{batch_idx:04d}.jsonl")
                    if os.path.exists(batch_file):
                        with open(batch_file, "r", encoding="utf-8") as in_f:
                            out_f.write(in_f.read())
                        # å¯é€‰ï¼šåˆ é™¤ä¸´æ—¶æ‰¹æ¬¡æ–‡ä»¶
                        # os.remove(batch_file)
            
            # 2. ç”ŸæˆJSONæ ¼å¼ï¼ˆä¾¿äºäººå·¥æŸ¥çœ‹ï¼‰
            merge_json = os.path.join(self.output_dir, "regenerated_complete.json")
            with open(merge_jsonl, "r", encoding="utf-8") as f:
                json_data = [json.loads(line.strip()) for line in f if line.strip()]
            with open(merge_json, "w", encoding="utf-8") as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æ‰€æœ‰æ‰¹æ¬¡åˆå¹¶å®Œæˆï¼š\n- JSONL: {merge_jsonl}\n- JSON: {merge_json}")
        except Exception as e:
            logger.error(f"åˆå¹¶æ‰¹æ¬¡å¤±è´¥ï¼š{e}")
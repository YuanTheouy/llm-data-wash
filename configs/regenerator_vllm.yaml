# vLLM服务配置
vllm:
  # --- API 配置 ---
  vllm_url: "http://localhost:8000/v1/chat/completions"
  model_name: "/workspace/Models/Llama-3.1-8B-Instruct"  # API调用时指定的模型名称 (通常就是模型路径)
  timeout: 60
  min_api_interval: 0.1

  # --- 生成参数 ---
  temperature: 0.7
  max_tokens: 2048
  
  # --- 兼容性保留 (如果需要) ---
  model_path: "/workspace/Models/Llama-3.1-8B-Instruct"

# 并行配置
concurrency:
  num_threads: 32     # API 请求并发线程数
  batch_size: 1000    # 批次保存大小
  batch_save: 10      # 每处理多少条保存一次 (对应 regen.py)

# 数据配置
data:
  input_path: "/workspace/datasets/ultrachat_200k-test_sft_sft_shard000.json"  # 输入数据集
  output_dir: "/workspace/regenerated_data_api"       # 输出目录
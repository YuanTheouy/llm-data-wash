# vLLM服务配置 - Qwen2.5-VL
vllm:
  # --- API 配置 ---
  vllm_url: "http://localhost:8000/v1/chat/completions"
  model_name: "/workspace/Models/Qwen2.5-VL-7B-Instruct"  # 服务端加载的模型路径
  timeout: 180            # 视觉模型推理较慢，建议调大超时时间
  min_api_interval: 0     # 设为0，全力发送请求

  # --- 生成参数 ---
  temperature: 0.1        # 视觉任务通常需要较低的temperature
  max_tokens: 1024        # 根据需要调整
  
# 并行配置
concurrency:
  num_threads: 32         # 4卡TP模式
  batch_save: 50          # 每处理50条保存一次

# 数据配置
data:
  input_path: "/workspace/datasets/VQAv2/vqa_v2_validation.jsonl"    # 输入数据集路径
  output_dir: "/workspace/datasets/VQAv2/qwen_output_temp"           # 临时输出目录（用于存放中间batch文件）
  final_output_path: "/workspace/datasets/VQAv2/vqa_v2_validation_answer.jsonl" # 最终合并后的输出文件路径
